ETL com PySpark, Pipeline com Apache Beam e execução pelo Job do Dataflow.

Arquivo pode ser obtido em https://www.kaggle.com/code/smiller933/basic-info-flights-csv-airports-csv-airlines-csv

Objetivos:

1 - Importar o arquivo a partir do datalake privado GCP

2 - ETL com PySpark 

3 - Gravar os arquivos tratados no datalake privado GCP

4 - Pipeline com Apache Beam nomodelo batch

